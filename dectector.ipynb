{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.util import random_noise\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with mask in training set : 663\n",
      "Number of samples without mask in training set : 661\n",
      "Number of samples with mask in test set : 97\n",
      "Number of samples without mask in test set : 97\n"
     ]
    }
   ],
   "source": [
    "#Some information about the number of records :\n",
    "print('Number of samples with mask in training set :',len(os.listdir('data/train/with_mask')))\n",
    "print('Number of samples without mask in training set :',len(os.listdir('data/train/without_mask')))\n",
    "print('Number of samples with mask in test set :',len(os.listdir('data/test/with_mask')))\n",
    "print('Number of samples without mask in test set :',len(os.listdir('data/test/without_mask')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Agumentation :  To increase the number of images.\n",
    "\n",
    "def h_flip(image) :\n",
    "    return np.fliplr(image)\n",
    "\n",
    "def v_flip(image) :\n",
    "    return np.flipud(image)\n",
    "\n",
    "def add_noise(image) :\n",
    "    return random_noise(image)\n",
    "\n",
    "def blur_image(image) :\n",
    "    return cv2.GaussianBlur(image,(7,7),0)\n",
    "\n",
    "\n",
    "transformations = {'horizontal flip' : h_flip,\n",
    "                   'vertical flip' : v_flip,\n",
    "                   'adding noise' : add_noise,\n",
    "                   'blur image ' : blur_image}\n",
    "\n",
    "\n",
    "def img_augumentation(img_path,aug_path,n_img) :\n",
    "    img = []\n",
    "\n",
    "    for im in os.listdir(img_path) :\n",
    "        img.append(os.path.join(img_path,im))\n",
    "         \n",
    "    i = 1\n",
    "    while(i <= n_img) :\n",
    "        image = random.choice(img)\n",
    "        original_img = mpimg.imread(image)\n",
    "        transformed_img = None\n",
    "        transform_key = random.choice(list(transformations)) \n",
    "        transformed_img =transformations[transform_key](original_img)\n",
    "\n",
    "        new_img_path =\"%s/aug_img_%s.jpg\" %(aug_path,i)\n",
    "        transformed_img = img_as_ubyte(transformed_img)\n",
    "        transformed_img = cv2.cvtColor(transformed_img,cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(new_img_path,transformed_img)\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augumentation('data/train/with_mask/','data/train/aug_with_mask',1000)\n",
    "img_augumentation('data/train/without_mask/','data/train/aug_without_mask',1000)\n",
    "img_augumentation('data/test/with_mask/','data/test/aug_with_mask',200)\n",
    "img_augumentation('data/test/without_mask/','data/test/aug_without_mask',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1324 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"data/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 142 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DIR = \"data/val\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-fb50dbaa623d>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 133 steps, validate for 15 steps\n",
      "Epoch 1/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6546 - acc: 0.6598WARNING:tensorflow:From C:\\Users\\I518759\\AppData\\Local\\Continuum\\anaconda3\\envs\\facemask\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "133/133 [==============================] - 110s 825ms/step - loss: 0.6524 - acc: 0.6616 - val_loss: 0.2864 - val_acc: 0.9155\n",
      "Epoch 2/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8546INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "133/133 [==============================] - 100s 753ms/step - loss: 0.3635 - acc: 0.8542 - val_loss: 0.1447 - val_acc: 0.9507\n",
      "Epoch 3/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.8912INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "133/133 [==============================] - 98s 740ms/step - loss: 0.2873 - acc: 0.8912 - val_loss: 0.1186 - val_acc: 0.9507\n",
      "Epoch 4/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9033INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "133/133 [==============================] - 102s 764ms/step - loss: 0.2577 - acc: 0.9033 - val_loss: 0.1081 - val_acc: 0.9648\n",
      "Epoch 5/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9072INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "133/133 [==============================] - 97s 728ms/step - loss: 0.2528 - acc: 0.9063 - val_loss: 0.1029 - val_acc: 0.9507\n",
      "Epoch 6/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9163INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "133/133 [==============================] - 103s 776ms/step - loss: 0.2261 - acc: 0.9162 - val_loss: 0.0755 - val_acc: 0.9718\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 95s 713ms/step - loss: 0.2190 - acc: 0.9192 - val_loss: 0.1657 - val_acc: 0.9437\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 94s 709ms/step - loss: 0.2316 - acc: 0.9109 - val_loss: 0.1506 - val_acc: 0.9366\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 94s 710ms/step - loss: 0.1928 - acc: 0.9252 - val_loss: 0.0760 - val_acc: 0.9930\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 101s 757ms/step - loss: 0.2414 - acc: 0.9056 - val_loss: 0.1771 - val_acc: 0.9296\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 97s 733ms/step - loss: 0.2313 - acc: 0.9275 - val_loss: 0.0912 - val_acc: 0.9859\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 95s 713ms/step - loss: 0.1758 - acc: 0.9426 - val_loss: 0.0909 - val_acc: 0.9648\n",
      "Epoch 13/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9444INFO:tensorflow:Assets written to: model-013.model\\assets\n",
      "133/133 [==============================] - 99s 741ms/step - loss: 0.1455 - acc: 0.9449 - val_loss: 0.0499 - val_acc: 0.9930\n",
      "Epoch 14/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9429INFO:tensorflow:Assets written to: model-014.model\\assets\n",
      "133/133 [==============================] - 100s 755ms/step - loss: 0.1657 - acc: 0.9418 - val_loss: 0.0479 - val_acc: 0.9930\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 107s 802ms/step - loss: 0.1658 - acc: 0.9434 - val_loss: 0.0803 - val_acc: 0.9718\n",
      "Epoch 16/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9376INFO:tensorflow:Assets written to: model-016.model\\assets\n",
      "133/133 [==============================] - 122s 916ms/step - loss: 0.1581 - acc: 0.9381 - val_loss: 0.0434 - val_acc: 0.9789\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 92s 688ms/step - loss: 0.2339 - acc: 0.9147 - val_loss: 0.0982 - val_acc: 0.9648\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 88s 659ms/step - loss: 0.1829 - acc: 0.9320 - val_loss: 0.0566 - val_acc: 0.9789\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 88s 664ms/step - loss: 0.1646 - acc: 0.9479 - val_loss: 0.0479 - val_acc: 0.9789\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 96s 722ms/step - loss: 0.1855 - acc: 0.9358 - val_loss: 0.0909 - val_acc: 0.9718\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 99s 744ms/step - loss: 0.1442 - acc: 0.9509 - val_loss: 0.0932 - val_acc: 0.9648\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 96s 722ms/step - loss: 0.1370 - acc: 0.9403 - val_loss: 0.1053 - val_acc: 0.9718\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 91s 685ms/step - loss: 0.1396 - acc: 0.9509 - val_loss: 0.0909 - val_acc: 0.9718\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 85s 642ms/step - loss: 0.1614 - acc: 0.9411 - val_loss: 0.0627 - val_acc: 0.9789\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 84s 632ms/step - loss: 0.1134 - acc: 0.9539 - val_loss: 0.0605 - val_acc: 0.9789\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 85s 636ms/step - loss: 0.1582 - acc: 0.9449 - val_loss: 0.0454 - val_acc: 0.9859\n",
      "Epoch 27/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9498INFO:tensorflow:Assets written to: model-027.model\\assets\n",
      "133/133 [==============================] - 91s 682ms/step - loss: 0.1398 - acc: 0.9502 - val_loss: 0.0385 - val_acc: 0.9930\n",
      "Epoch 28/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9559INFO:tensorflow:Assets written to: model-028.model\\assets\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 0.1161 - acc: 0.9547 - val_loss: 0.0375 - val_acc: 0.9930\n",
      "Epoch 29/30\n",
      "133/133 [==============================] - 93s 697ms/step - loss: 0.1717 - acc: 0.9366 - val_loss: 0.0585 - val_acc: 0.9718\n",
      "Epoch 30/30\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9566INFO:tensorflow:Assets written to: model-030.model\\assets\n",
      "133/133 [==============================] - 115s 865ms/step - loss: 0.1247 - acc: 0.9569 - val_loss: 0.0356 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle-mixin\n",
      "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
      "Building wheels for collected packages: pickle-mixin\n",
      "  Building wheel for pickle-mixin (setup.py): started\n",
      "  Building wheel for pickle-mixin (setup.py): finished with status 'done'\n",
      "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6003 sha256=3b862e165beac1141e414def10eaa7f2536cb894230eb2b690857458d0df07a4\n",
      "  Stored in directory: c:\\users\\i518759\\appdata\\local\\pip\\cache\\wheels\\82\\53\\b0\\6f80da2d461fa5f582eb274b0158ce81d01b977cbb59a2ae6a\n",
      "Successfully built pickle-mixin\n",
      "Installing collected packages: pickle-mixin\n",
      "Successfully installed pickle-mixin-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#! pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: face_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#saving the model in pickel\n",
    "import pickle\n",
    "\n",
    "#pickle.save(model, open('facemask_model.sav', 'wb'))\n",
    "model.save(\"face_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f8389674b4fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# Show the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels_dict={0:'with0ut_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "    \n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
